---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 
We use (`*`) to indicate a problem that we think might be time consuming. 
    
## Style Points (10 pts) 
Please refer to the minilesson on code style
**[here](https://uchicago.zoom.us/rec/share/pG_wQ-pHTQrJTmqNn4rcrw5V194M2H2s-2jdy8oVhWHkd_yZt9o162IWurpA-fxU.BIQlSgZLRYctvzp-)**.

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID):
    - Partner 2 (name and cnet ID):
3. Partner 1 will accept the `ps4` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*\_\_\*\* \*\*\_\_\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*\_\_\*\* Late coins left after submission: \*\*\_\_\*\*
7. Knit your `ps4.qmd` to an PDF file to make `ps4.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps4.qmd` and `ps4.pdf` to your github repo.
9. (Partner 1): submit `ps4.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

**Important:** Repositories are for tracking code. **Do not commit the data or shapefiles to your repo.** The best way to do this is with `.gitignore`, which we have covered in class. If you do accidentally commit the data, Github has a [guide](https://docs.github.com/en/repositories/working-with-files/managing-large-files/about-large-files-on-github#removing-files-from-a-repositorys-history). The best course of action depends on whether you have pushed yet. This also means that both partners will have to download the initial raw data and any data cleaning code will need to be re-run on both partners' computers. 

## Download and explore the Provider of Services (POS) file (10 pts)

1. 
2. 
4. 
    a.
    b.
## Identify hospital closures in POS file (15 pts) (*)
1. Use this definition to create a list of all hospitals that were active in 2016 that were suspected to have closed by 2019. Record the facility name and zip of each hospital as well as the year of suspected closure (when they become terminated or disappear from the data). How many hospitals are there that fit this definition?
2. First 10 rows of the cleaned 2016 dataset:
   PRVDR_CTGRY_SBTYP_CD  PRVDR_CTGRY_CD                          FAC_NAME  \
0                   1.0               1  SOUTHEAST ALABAMA MEDICAL CENTER   
1                   1.0               1            NORTH JACKSON HOSPITAL   
2                   1.0               1     MARSHALL MEDICAL CENTER SOUTH   
3                   1.0               1    ELIZA COFFEE MEMORIAL HOSPITAL   
4                   1.0               1          MIZELL MEMORIAL HOSPITAL   
5                   1.0               1       CRENSHAW COMMUNITY HOSPITAL   
6                   1.0               1          HARTSELLE MEDICAL CENTER   
7                   1.0               1     MARSHALL MEDICAL CENTER NORTH   
8                   1.0               1                 ST VINCENT'S EAST   
9                   1.0               1    DEKALB REGIONAL MEDICAL CENTER 
3. 
    a. .Number of remaining closures after correcting for mergers/acquisitions: 77
    b.Cleaned 2016 dataset saved as 'cleaned_2016_pos.csv' without suspected closures
    c.
                                              FAC_NAME   ZIP_CD  \
62                             ALLIANCE LAIRD HOSPITAL  39365.0   
101                           ALLIANCEHEALTH DEACONESS  73112.0   
26                       ANNE BATES LEACH EYE HOSPITAL  33136.0   
115                      BARIX CLINICS OF PENNSYLVANIA  19047.0   
171                    BAYLOR EMERGENCY MEDICAL CENTER  75087.0   
166  BAYLOR SCOTT & WHITE EMERGENCY MEDICAL CENTER ...  78613.0   
98                          BELMONT COMMUNITY HOSPITAL  43906.0   
67                              BIG SKY MEDICAL CENTER  59716.0   
65                BLACK RIVER COMMUNITY MEDICAL CENTER  63901.0   
142                       CARE REGIONAL MEDICAL CENTER  78336.0   
```{python}
import pandas as pd
pos2016 = hospital_data('/Users/jxn/Documents/GitHub/Alex-Nelly/pos2016.csv')
pos2017 = hospital_data('/Users/jxn/Documents/GitHub/Alex-Nelly/pos2017.csv')
pos2018 = hospital_data('/Users/jxn/Documents/GitHub/Alex-Nelly/pos2018.csv')
pos2019 = hospital_data('/Users/jxn/Documents/GitHub/Alex-Nelly/pos2019.csv')

hospital_active = pos2016[pos2016['PGM_TRMNTN_CD'] == '0']
data_years = {2017: pos2017, 2018: pos2018, 2019: pos2019}

# Step 2: Detect suspected closures
hospital_closures = []
for _, row in hospital_active.iterrows():
    prvid = row['PRVDR_NUM']
    facility_name = row['FAC_NAME']
    zip_code = row['ZIP_CD']

    closed = False
    for year, data in data_years.items():
        if prvid in data['PRVDR_NUM'].values:
            status = data[data['PRVDR_NUM'] == prvid]['PGM_TRMNTN_CD'].values[0]
            if status != '0':
                hospital_closures.append((facility_name, zip_code, year))
                closed = True
                break
        else:
            hospital_closures.append((facility_name, zip_code, year))
            closed = True
            break

# Convert suspected closures to DataFrame
closures_df = pd.DataFrame(hospital_closures, columns=['FAC_NAME', 'ZIP_CD', 'Year_of_Closure']).drop_duplicates()

# Step 3: Identify potential mergers/acquisitions
potential_mergers = []
for _, row in closures_df.iterrows():
    zip_code = row['ZIP_CD']
    year_closure = row['Year_of_Closure']
    if year_closure < 2019:
        active_current_year = data_years[year_closure][data_years[year_closure]['PGM_TRMNTN_CD'] == '0'].groupby('ZIP_CD').size()
        active_next_year = data_years[year_closure + 1][data_years[year_closure + 1]['PGM_TRMNTN_CD'] == '0'].groupby('ZIP_CD').size()
        
        if active_next_year.get(zip_code, 0) >= active_current_year.get(zip_code, 0):
            potential_mergers.append(row)

potential_mergers_df = pd.DataFrame(potential_mergers)

# Step 4: Exclude potential mergers from suspected closures
final_closures_df = pd.merge(closures_df, potential_mergers_df[['FAC_NAME', 'ZIP_CD', 'Year_of_Closure']], 
                             on=['FAC_NAME', 'ZIP_CD', 'Year_of_Closure'], how='left', indicator=True)
final_closures_df = final_closures_df[final_closures_df['_merge'] == 'left_only'].drop(columns=['_merge'])

num_final_closures = final_closures_df.shape[0]
print(f"Number of remaining closures after correcting for mergers/acquisitions: {num_final_closures}")
# Sort by hospital name and display the first 10 rows
sorted_final_closures_df = final_closures_df.sort_values(by='FAC_NAME').head(10)
print(sorted_final_closures_df)
final_closures_df.to_csv("final_cleaned_Data.csv",index=False)

# Step 5: Clean the 2016 data by excluding true closures
cleaned_2016_pos = pos2016.merge(final_closures_df[['FAC_NAME', 'ZIP_CD']], on=['FAC_NAME', 'ZIP_CD'], how='left', indicator=True)
cleaned_2016_pos = cleaned_2016_pos[cleaned_2016_pos['_merge'] == 'left_only'].drop(columns=['_merge'])
cleaned_2016_pos.to_csv("cleaned_2016_pos.csv", index=False)
print("Cleaned 2016 dataset saved as 'cleaned_2016_pos.csv' without suspected closures.")
print("First 10 rows of the cleaned 2016 dataset:")
print(cleaned_2016_pos.head(10))

```

## Download Census zip code shapefile (10 pt) 
1. (Partner 2) Create a GeoDataFrame for the centroid of each zip code nationally: zips_all_centroids. What are the dimensions of the resulting GeoDataFrame and what do each of the columns mean?
2. (Partner 2) Create two GeoDataFrames as subsets of zips_all_centroids. First, create all zip codes in Texas: zips_texas_centroids. Then, create all zip codes in Texas or a bordering state: zips_texas_borderstates_centroids, using the zip code prefixes to make these subsets. How many unique zip codes are in each of these subsets?
3. (Partner 2) Then create a subset of zips_texas_borderstates_centroids that con- tains only the zip codes with at least 1 hospital in 2016. Call the resulting Geo- DataFrame zips_withhospital_centroids What kind of merge did you decide to do, and what variable are you merging on?
4. (Partner 2) For each zip code in zips_texas_centroids, calculate the distance to the nearest zip code with at least one hospital in zips_withhospital_centroids.
a. This is a computationally-intensive join. Before attempting to do the entire join, subset to 10 zip codes in zips_texas_centroids and try the join. How long did it take? Approximately how long do you estimate the entire procedure will take?
b. Now try doing the full calculation and time how long it takes. How close is it to your estimation?
c. Look into the .prj file and report which unit it is in. Convert the given unit to miles, using an appropriate conversion you find online (estimates are okay).
5. (Partner 2) Calculate the average distance to the nearest hospital for each zip code in Texas.
a. What unit is this in?
b. Report the average distance in miles. Does this value make sense?
c. Map the value for each zip code.
`
```{python}
import geopandas as gpd

```
```{python}

# Load the shapefile of ZIP codes (replace 'path_to_shapefile' with the actual path)

zips_all = gpd.read_file('/Users/jxn/Desktop/gz_2010_us_860_00_500k')
zips_all_centroids = zips_all.copy()
zips_all_centroids['centroid'] = zips_all.geometry.centroid
zips_all_centroids = zips_all_centroids.set_geometry('centroid')
print("Available columns in zips_centroids:", zips_centroids.columns)
print(" columns in zips_centroids:", zips_all_centroids.columns)
print("Dimensions of zips_all_centroids:", zips_all_centroids.shape)
print("Columns in zips_all_centroids:", zips_all_centroids.columns)

# Texas ZIP codes 
zips_texas_centroids = zips_all_centroids[zips_all_centroids['STATEFP'] == '48']
border_state_prefixes = ['75', '76', '77', '78', '79', '73', '88', '89', '84', '85', '86', '87']
zips_texas_borderstates_centroids = zips_all_centroids[
    zips_all_centroids['ZIPCODE'].str[:2].isin(border_state_prefixes)
]
print("Unique ZIP codes in Texas:", zips_texas_centroids['ZIPCODE'].nunique())
print("Unique ZIP codes in Texas and bordering states:", zips_texas_borderstates_centroids['ZIPCODE'].nunique())
pos2016_with_hospitals = pos2016[pos2016['PGM_TRMNTN_CD'] == '0'][['ZIP_CD']].drop_duplicates()
pos2016_with_hospitals.columns = ['ZIPCODE']
zips_withhospital_centroids = zips_texas_borderstates_centroids.merge(
    pos2016_with_hospitals, on='ZIPCODE', how='inner'
)

print("Number of ZIP codes with at least one hospital in 2016:", zips_withhospital_centroids['ZIPCODE'].nunique())
from shapely.geometry import Point
import time
test_zips = zips_texas_centroids.sample(10)
start_time = time.time()
test_zips['nearest_hospital_distance'] = test_zips.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)
end_time = time.time()
print("Time taken for 10 ZIP codes:", end_time - start_time, "seconds")
estimated_time = (end_time - start_time) * (len(zips_texas_centroids) / 10)
print("Estimated time for full dataset:", estimated_time, "seconds")
start_time_full = time.time()
zips_texas_centroids['nearest_hospital_distance'] = zips_texas_centroids.geometry.apply(
    lambda x: zips_withhospital_centroids.distance(x).min()
)
end_time_full = time.time()

print("Actual time for full calculation:", end_time_full - start_time_full, "seconds")
zips_texas_centroids['nearest_hospital_distance_miles'] = zips_texas_centroids['nearest_hospital_distance'] * 0.000621371
average_distance_miles = zips_texas_centroids['nearest_hospital_distance_miles'].mean()
print("Average distance to nearest hospital (in miles):", average_distance_miles)
zips_texas_centroids.plot(column='nearest_hospital_distance_miles', legend=True, cmap='viridis')
plt.title("Distance to Nearest Hospital in Miles for Texas ZIP Codes")
plt.xlabel("Longitude")
plt.ylabel("Latitude")
plt.show()
```


    a.
    b.
    c.

## Effects of closures on access in Texas (15 pts)

1. 
2. 
3. 
4. 

## Reflecting on the exercise (10 pts) 

2. Our current method is a good place to start, but we could get a more complete picture of ZIP-code-level hospital access by adding factors like distance, population, area demand, and capacity. This more detailed study would help us get a better picture of how real changes in access are caused by hospital closings.Our current method is a good place to start, but we could get a more complete picture of ZIP-code-level hospital access by adding factors like distance, population, area demand, and capacity. This more detailed study would help us get a better picture of how real changes in access are caused by hospital closings.